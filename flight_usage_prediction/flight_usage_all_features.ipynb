{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tzfpy import get_tz\n",
    "from pytz import timezone\n",
    "import kds\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ydata_profiling as df_report\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sweetviz as sv\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import h3\n",
    "import keplergl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from private dataset and public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usage = pd.read_csv('data/usage_data_2023_05_01_to_15.csv')\n",
    "display(df_usage.head(2))\n",
    "df_airtraffic_trends = pd.read_csv('data/Airline_Transportation_Data.csv')\n",
    "display(df_airtraffic_trends.head(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing column re-names, EDA,Data Cleaning, Feature Engineering and Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_airtraffic_trends.rename(columns={'OBS_DATE': 'Month','ASM_US_D_I':'Aggr_seat_miles_domestic_and_international','LOAD_FACTOR_D_PCT_US':'Load_factor_domestic_pct_us'}, inplace=True)\n",
    "df_airtraffic_trends.drop(['ID','ASM_D','ASM_I','LOAD_FACTOR_I_PCT_US'], axis=1, inplace=True)\n",
    "df_airtraffic_trends['Month'] = pd.to_datetime(df_airtraffic_trends['Month'])\n",
    "df_usage.columns = df_usage.columns.str.lower()\n",
    "df_airtraffic_trends.columns = df_airtraffic_trends.columns.str.lower()\n",
    "df_usage['usage_tstamp'] = pd.to_datetime(df_usage['usage_tstamp'])\n",
    "df_usage['door_close_tstamp'] = pd.to_datetime(df_usage['door_close_tstamp'])\n",
    "df_usage['door_open_plus15_tstamp'] = pd.to_datetime(df_usage['door_open_plus15_tstamp'])\n",
    "\n",
    "# Based on several iterations of the EDA, it felt like I need to come up with a new feature which is percentage of flight completed \n",
    "# and need to find all time related variables in local time than in utc. Used lat,lon to convert time to local timezone.\n",
    "# Dropped all rows with null vallues for lat,long\n",
    "\n",
    "df_usage['pct_flight_completed'] = df_usage['time_into_flight']/((df_usage['door_open_plus15_tstamp'] - df_usage['door_close_tstamp']).dt.total_seconds() / 60)\n",
    "df_usage.drop(['tail_id','door_close_tstamp','door_open_plus15_tstamp'], axis=1, inplace=True)\n",
    "df_usage.dropna(subset=['latitude','longitude','seat_capacity','eco_seats','airline'],inplace=True)\n",
    "df_usage['free_model'] = df_usage['airline'].apply(lambda x: 1 if x in ['DAL','JBU','JBU-R','QFA'] else 0)\n",
    "# The initial data set had a lot of datapoints, so I sampled 1% of the data to do the EDA and build the model\n",
    "df_usage_sampled = df_usage.sample(frac=0.5, random_state=42,replace=False)\n",
    "\n",
    "display(df_airtraffic_trends.head(2))\n",
    "display(df_usage_sampled.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing timezone trasnlations to get local time as well as hour of day and day of week\n",
    "\n",
    "df_usage_sampled['usage_utc_hour'] = df_usage_sampled['usage_tstamp'].dt.hour\n",
    "df_usage_sampled['usage_month']=df_usage_sampled['usage_tstamp'].dt.month_name()\n",
    "df_usage_sampled['usage_utc_day']=df_usage_sampled['usage_tstamp'].dt.day_name()\n",
    "\n",
    "def local_time_mapper(x):\n",
    "    return get_tz(lat=x['latitude'], lng=x['longitude'])\n",
    "\n",
    "\n",
    "df_usage_sampled['usage_tz'] = df_usage_sampled.apply(lambda x: local_time_mapper(x), axis=1)\n",
    "\n",
    "# Throwing away few rows which had no timezone look up, very small numebr of rows\n",
    "df_usage_sampled = df_usage_sampled[df_usage_sampled['usage_tz']!='']\n",
    "\n",
    "# Converting utc time to local time based on the timezone and then getting the local hour of the day and day of the week\n",
    "\n",
    "df_usage_sampled['usage_local_hour'] = df_usage_sampled.apply(lambda row: row['usage_tstamp'].tz_convert(timezone(row['usage_tz'])).hour, axis=1)\n",
    "df_usage_sampled['usage_local_hour_shifted'] = df_usage_sampled['usage_local_hour'].apply(lambda x: (pd.Timedelta(hours=x) + pd.Timedelta(hours=-3)).components.hours)\n",
    "df_usage_sampled['usage_local_day'] = df_usage_sampled.apply(lambda row: row['usage_tstamp'].tz_convert(timezone(row['usage_tz'])).strftime(\"%A\"), axis=1)\n",
    "\n",
    "display(df_usage_sampled.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping un-wanted rows before doing one more final round of eda through sweetviz\n",
    "\n",
    "df_usage_sampled.drop(['usage_tstamp','usage_tz'], axis=1, inplace=True)\n",
    "\n",
    "df_usage_eda_report = sv.analyze(df_usage_sampled, target_feat='total_fl_mbps')\n",
    "df_usage_eda_report.show_html('eda-viz/df_usage_per_tail_seatcapacity_eda_report.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the output of the FInal run of eda, dropping unwanted columns as well as columns which are not available at the time of prediction ('total_rl_mbps')\n",
    "df_usage_sampled.drop(columns=['usage_utc_day','usage_utc_hour','usage_local_hour','usage_month','total_rl_mbps'],inplace=True)\n",
    "df_usage_sampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting local_hour and pct flight completed to categorical variables based on the output of final eda run\n",
    "df_usage_sampled['usage_local_hour_shifted'] = df_usage_sampled['usage_local_hour_shifted'].astype('str')\n",
    "df_usage_sampled['pct_flight_completed'] = round((100*df_usage_sampled['pct_flight_completed']/10),0).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#throwing outliers in total_fl_mbps which are between 0.1 and  99.5th percentile\n",
    "df_usage_sampled = df_usage_sampled[ ( df_usage_sampled['total_fl_mbps']<df_usage_sampled['total_fl_mbps'].quantile(0.995)) & (df_usage_sampled['total_fl_mbps']>df_usage_sampled['total_fl_mbps'].quantile(0.001)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying column transformer to encode categorical variables and scale numeric variables\n",
    "\n",
    "# Get columns of integer or float type\n",
    "numeric_cols = df_usage_sampled.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(numeric_cols)\n",
    "# Get columns of object type\n",
    "object_cols = df_usage_sampled.select_dtypes(include=['object']).columns\n",
    "print(object_cols)\n",
    "\n",
    "# Categorical feature columns to one-hot encode\n",
    "categorical_cols = ['airline','flight_phase','usage_local_day','pct_flight_completed','usage_local_hour_shifted','pct_flight_completed']\n",
    "# Numerical feature columns to standard scale\n",
    "numerical_cols = ['altitude','seat_capacity','eco_seats']\n",
    "\n",
    "# Create transformers for one-hot encoding and standard scaling\n",
    "column_trans = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ohe',OneHotEncoder(sparse=False), categorical_cols),\n",
    "        ('std_scaled',StandardScaler(), numerical_cols),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    )\n",
    "column_trans.set_output(transform='pandas')\n",
    "transformed_data = column_trans.fit_transform(df_usage_sampled)\n",
    "df_usage_column_transformed = pd.DataFrame(transformed_data)\n",
    "\n",
    "df_usage_column_transformed.rename(columns={'remainder__total_fl_mbps':'total_fl_mbps'}, inplace=True)\n",
    "display(df_usage_column_transformed.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_before_poly = df_usage_column_transformed.drop('total_fl_mbps', axis=1) \n",
    "# display(X_before_poly.shape)\n",
    "# display(X_before_poly.head())\n",
    "# poly_features = PolynomialFeatures(degree=2)\n",
    "# X_poly = poly_features.fit_transform(X_before_poly)\n",
    "# X = pd.DataFrame(X_poly, columns=poly_features.get_feature_names_out(X_before_poly.columns))\n",
    "\n",
    "# display(X.shape)\n",
    "# display(X.head())\n",
    "display(df_usage_column_transformed.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df into train test split with 80:20 ratio \n",
    "\n",
    "X = df_usage_column_transformed.loc[:, df_usage_column_transformed.columns != 'total_fl_mbps']\n",
    "y = df_usage_column_transformed['total_fl_mbps']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining a set of regression models to be used for hyper parameter tuning\n",
    "\n",
    "regressors = {\n",
    "    'Linear Regression': (LinearRegression(), {}),\n",
    "    'Ridge Regression': (Ridge(fit_intercept=False), {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}),\n",
    "    'Lasso Regression': (Lasso(), {'alpha': [0.00001,0.0001,0.001,0.01, 0.1, 1.0, 10.0, 100.0]}),\n",
    "    'Random Forest Regressor': (RandomForestRegressor(n_jobs=-1), {'n_estimators': [ 50, 100, 200], 'max_depth': [5, 7, 10, 15, 20, 40]}),\n",
    "    #'Support Vector Regressor': (SVR(), {'C': [1, 10], 'kernel': ['linear', 'rbf']})\n",
    "}\n",
    "\n",
    "best_models = []\n",
    "# Perform grid search for each regressor\n",
    "for regressor_name, (regressor, param_grid) in regressors.items():\n",
    "    grid_search = GridSearchCV(regressor, param_grid, scoring='r2', cv=5, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best estimator and make predictions\n",
    "    best_regressor = grid_search.best_estimator_\n",
    "    y_pred = best_regressor.predict(X_test)\n",
    "\n",
    "    # Calculate and print the R2 score\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{regressor_name}: Best Parameters: {grid_search.best_params_}, R2 Score: {r2}\")\n",
    "    best_models.append({'regressor':regressor_name,'model':best_regressor})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(best_models[3]['regressor'])\n",
    "display(best_models[3]['model'].feature_importances_)\n",
    "importances = best_models[3]['model'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "sorted_importances = importances[indices]\n",
    "sorted_features = X_test.columns[indices]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X_test.shape[1]), sorted_importances, align=\"center\")\n",
    "plt.xticks(range(X_test.shape[1]), sorted_features, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "# Print mean absolute percentage error (MAPE), R2 score, and mean absolute error (MAE)\n",
    "\n",
    "\n",
    "best_regressor = best_models[3]['model']\n",
    "y_pred = best_regressor.predict(X_test)\n",
    "\n",
    "# Calculate and print the MAPE score\n",
    "mape = round(mean_absolute_percentage_error(y_test, y_pred),2)\n",
    "mae = round(mean_absolute_error(y_test, y_pred),2)\n",
    "mse = round(mean_squared_error(y_test, y_pred),2)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Best Regressor's Scores: MAPE : {mape}, MAE : {mae}, MSE : {mse}, R2 : {r2}\".format(mape=mape,mse=mse,mae=mae,r2=r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Predicted vs Actual\")\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
